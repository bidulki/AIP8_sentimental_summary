{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "381a9EnQvAFA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path_baseline = 'result_baseline.tsv'\n",
        "file_path_ratio82 = 'result_ratio_8_2.tsv'\n",
        "\n",
        "baseline_df = pd.read_csv(file_path_baseline, sep='\\t')\n",
        "adjusted_df = pd.read_csv(file_path_ratio82, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(baseline_df.head())\n",
        "print(adjusted_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iuU3KDvvWsK",
        "outputId": "ec0f65a2-7b42-4d26-9f07-146baf1e5d58"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         original             summary\n",
            "0  describe the taste in one word                 not\n",
            "1                 favorite coffee  my favorite coffee\n",
            "2            yummy coconut flavor        great coffee\n",
            "3                        so yummy         great candy\n",
            "4    teeccino herbal coffee mocha        great coffee\n",
            "                         original       summary\n",
            "0  describe the taste in one word    not stevia\n",
            "1                 favorite coffee  great coffee\n",
            "2            yummy coconut flavor       love it\n",
            "3                        so yummy          zotz\n",
            "4    teeccino herbal coffee mocha  great coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adjusted 데이터프레임의 summary 열에 새로운 이름 부여\n",
        "adjusted_df = adjusted_df.rename(columns={'summary': 'summary_adjusted'})\n",
        "\n",
        "# baseline 데이터프레임에 adjusted 데이터프레임의 summary_adjusted 열 추가\n",
        "merged_df = pd.concat([baseline_df, adjusted_df['summary_adjusted']], axis=1)\n",
        "\n",
        "# 결과 확인\n",
        "print(merged_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np7wn0JFvd4Q",
        "outputId": "7c0d00a6-372c-4dc2-a082-26fce725e1d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         original             summary summary_adjusted\n",
            "0  describe the taste in one word                 not       not stevia\n",
            "1                 favorite coffee  my favorite coffee     great coffee\n",
            "2            yummy coconut flavor        great coffee          love it\n",
            "3                        so yummy         great candy             zotz\n",
            "4    teeccino herbal coffee mocha        great coffee     great coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#====================================================================#"
      ],
      "metadata": {
        "id": "nMGc6aIqL_Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "def calculate_cosine_similarity(vec1, vec2, chunk_size=100):\n",
        "    similarity_scores = []\n",
        "    for i in range(0, vec1.shape[0], chunk_size):\n",
        "        chunk_end = min(i + chunk_size, vec1.shape[0])\n",
        "        chunk_similarity = 1 - pairwise_distances(vec1[i:chunk_end], vec2[i:chunk_end], metric='cosine')\n",
        "        similarity_scores.extend(chunk_similarity)\n",
        "    return np.array(similarity_scores)\n",
        "\n",
        "# 원본 리뷰, summary, adjust summary를 각각 리스트로 가져옴\n",
        "original_reviews = merged_df['original'].tolist()\n",
        "summaries = merged_df['summary'].tolist()\n",
        "adjusted_summaries = merged_df['summary_adjusted'].tolist()\n",
        "\n",
        "# 데이터를 무작위로 섞고, 일부만 샘플링\n",
        "sample_size = 100  # 원하는 샘플 크기로 조정\n",
        "merged_df_sampled = merged_df.sample(sample_size, random_state=42)\n",
        "\n",
        "original_reviews_sampled = merged_df_sampled['original'].tolist()\n",
        "summaries_sampled = merged_df_sampled['summary'].tolist()\n",
        "adjusted_summaries_sampled = merged_df_sampled['summary_adjusted'].tolist()\n",
        "\n",
        "# CountVectorizer를 사용하여 텍스트를 벡터로 변환\n",
        "vectorizer = CountVectorizer().fit(original_reviews_sampled + summaries_sampled + adjusted_summaries_sampled)\n",
        "original_vectors_sampled = vectorizer.transform(original_reviews_sampled)\n",
        "summary_vectors_sampled = vectorizer.transform(summaries_sampled)\n",
        "adjusted_summary_vectors_sampled = vectorizer.transform(adjusted_summaries_sampled)\n",
        "\n",
        "# \"original\"와 \"summary\" 간의 코사인 유사성 계산\n",
        "similarity_scores_summary = calculate_cosine_similarity(original_vectors_sampled, summary_vectors_sampled)\n",
        "\n",
        "# \"original\"와 \"adjusted summary\" 간의 코사인 유사성 계산\n",
        "similarity_scores_adjusted = calculate_cosine_similarity(original_vectors_sampled, adjusted_summary_vectors_sampled)\n",
        "\n",
        "# 각 리뷰에 대한 정보 손실 평가: 1에 가까울수록 손실이 적음\n",
        "information_loss_summary = 1 - np.diag(similarity_scores_summary)\n",
        "information_loss_adjusted = 1 - np.diag(similarity_scores_adjusted)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Summary의 샘플 리뷰 평균 정보 손실:\", information_loss_summary.mean())\n",
        "print(\"Adjusted Summary의 샘플 리뷰 평균 정보 손실:\", information_loss_adjusted.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twhj6hI6M1sn",
        "outputId": "70b79537-d2ca-4d87-d41f-b1cac591f940"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary의 샘플 리뷰 평균 정보 손실: 0.866460522502189\n",
            "Adjusted Summary의 샘플 리뷰 평균 정보 손실: 0.8420345245045597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import edit_distance\n",
        "\n",
        "# 각 리뷰에 대한 편집 거리 계산\n",
        "merged_df['edit_distance_summary'] = merged_df.apply(lambda row: edit_distance(row['original'], row['summary']), axis=1)\n",
        "merged_df['edit_distance_adjusted'] = merged_df.apply(lambda row: edit_distance(row['original'], row['summary_adjusted']), axis=1)\n",
        "\n",
        "# 편집 거리를 정보 손실로 해석 (편집 거리가 크면 정보 손실이 크다고 가정)\n",
        "max_edit_distance = max(merged_df['edit_distance_summary'].max(), merged_df['edit_distance_adjusted'].max())\n",
        "merged_df['information_loss_summary'] = merged_df['edit_distance_summary'] / max_edit_distance\n",
        "merged_df['information_loss_adjusted'] = merged_df['edit_distance_adjusted'] / max_edit_distance\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Summary의 전체 리뷰 평균 정보 손실:\", merged_df['information_loss_summary'].mean())\n",
        "print(\"Adjusted Summary의 전체 리뷰 평균 정보 손실:\", merged_df['information_loss_adjusted'].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8hxVbWGUqGN",
        "outputId": "6ce9dcb0-132e-4053-ec95-0b9d86b968d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary의 전체 리뷰 평균 정보 손실: 0.1892795329210602\n",
            "Adjusted Summary의 전체 리뷰 평균 정보 손실: 0.16006387637817113\n"
          ]
        }
      ]
    }
  ]
}